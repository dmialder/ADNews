from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import time 

start = time.time()
model_id = "deepseek-ai/deepseek-llm-7b-chat"

tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)

model = AutoModelForCausalLM.from_pretrained(
    model_id,
    trust_remote_code=True,
    device_map={"": "cpu"}  # –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –≤—Å—ë –≤ CPU
)

pipe = pipeline("text-generation", model=model, tokenizer=tokenizer)

print("mid_works = " + str(time.time() - start))

prompt = (
    "–°–¥–µ–ª–∞–π –∫—Ä–∞—Ç–∫—É—é –∏—Å—Ç–æ—Ä–∏—é –∏–∑ —Å–ª–µ–¥—É—é—â–µ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è:\n"
    "–ú–∞–∫—Å–∏–º –ø–æ—Ç–µ—Ä—è–ª —Ç–∞–ø–æ–∫.\n"
)

res = pipe(prompt, max_new_tokens=500, do_sample=False)[0]['generated_text']
summary = res.split("<|assistant|>")[-1].strip()

print("\nüìÑ –°–≤–æ–¥–∫–∞ –æ—Ç DeepSeek:\n", summary)

print("finish")
print(time.time() - start)